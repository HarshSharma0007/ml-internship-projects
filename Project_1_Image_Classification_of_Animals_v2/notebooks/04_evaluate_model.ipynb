{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c661c08",
   "metadata": {},
   "source": [
    "### 1. **Setup + Imports Cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 🔁 Add your project root to Python path\n",
    "project_root = os.path.abspath(\"..\")  # or \"../ProjectName\" if nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "# Now you can import modules from the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from src.dataloader import get_dataloaders\n",
    "from src.models.resnet import get_model  # or use build_effnet if swapping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca1144",
   "metadata": {},
   "source": [
    "### 2. **Load Latest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model(path=\"../models\", pattern=\"*.pt\"):\n",
    "    files = glob.glob(os.path.join(path, pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .pt files found in: {os.path.abspath(path)}\")\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = get_latest_model()\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d35f7d",
   "metadata": {},
   "source": [
    "### 3. **Inference + Accuracy on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader = get_dataloaders()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        preds = model(x)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"🎯 Test Accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b06978",
   "metadata": {},
   "source": [
    "### 4. **Confusion Matrix + Save to `reports/`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_confusion_matrix(model, dataloader, class_names, save_path=\"../reports/confusion_matrix.png\"):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            preds.extend(predicted.cpu().tolist())\n",
    "            targets.extend(y.tolist())\n",
    "\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    # Ensure reports directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Plot and save the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp.plot(cmap=\"Blues\", xticks_rotation=45, values_format='d')\n",
    "    plt.title(\"📊 Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✅ Saved confusion matrix to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d865c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run predictions over test set\n",
    "all_preds, all_labels = [], []\n",
    "class_names = [\n",
    "    \"Bear\", \"Bird\", \"Cat\", \"Cow\", \"Deer\", \"Dog\", \"Dolphin\",\n",
    "    \"Elephant\", \"Giraffe\", \"Horse\", \"Kangaroo\", \"Lion\", \"Panda\",\n",
    "    \"Tiger\", \"Zebra\"\n",
    "]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "# 2. Compute metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, labels=range(len(class_names)), zero_division=0\n",
    ")\n",
    "\n",
    "# 3. Build dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    \"Class\": class_names,\n",
    "    \"Images\": support,\n",
    "    \"Precision\": np.round(precision, 4),\n",
    "    \"Recall\": np.round(recall, 4),\n",
    "    \"F1-Score\": np.round(f1, 4)\n",
    "})\n",
    "\n",
    "# 4. Display it\n",
    "print(\"📊 Per-Class Evaluation Metrics\")\n",
    "display(results_df)\n",
    "\n",
    "# 5. Save to CSV\n",
    "results_df.to_csv(\"../reports/per_class_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df24ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "_, _, test_loader = get_dataloaders()\n",
    "\n",
    "class_names = [\n",
    "    \"Bear\", \"Bird\", \"Cat\", \"Cow\", \"Deer\", \"Dog\", \"Dolphin\",\n",
    "    \"Elephant\", \"Giraffe\", \"Horse\", \"Kangaroo\", \"Lion\", \"Panda\",\n",
    "    \"Tiger\", \"Zebra\"\n",
    "]\n",
    "\n",
    "generate_and_save_confusion_matrix(model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaef75",
   "metadata": {},
   "source": [
    "### 5. **Plot Metrics from MLflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65de612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_acc, val_acc, save_path=\"../reports/final_accuracy_plot.png\"):\n",
    "    epochs = list(range(1, len(train_acc) + 1))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy', marker='o', color='seagreen')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy', marker='s', color='darkred')\n",
    "\n",
    "    plt.title(\"📈 Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    " \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(f\"✅ Accuracy plot saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec396024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"animal_classifier_v2\")\n",
    "# run = sorted(client.search_runs(experiment.experiment_id), key=lambda r: r.start_time)[-1]\n",
    "run = sorted(\n",
    "    client.search_runs([experiment.experiment_id]),\n",
    "    key=lambda r: r.info.start_time\n",
    ")[-1]\n",
    "\n",
    "run_id = run.info.run_id\n",
    "\n",
    "train_acc = [m.value for m in client.get_metric_history(run_id, \"train_acc\")]\n",
    "val_acc = [m.value for m in client.get_metric_history(run_id, \"val_acc\")]\n",
    "\n",
    "plot_accuracy(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c3422",
   "metadata": {},
   "source": [
    "### 6. **Visualize Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"Bear\", \"Bird\", \"Cat\", \"Cow\", \"Deer\", \"Dog\", \"Dolphin\",\n",
    "    \"Elephant\", \"Giraffe\", \"Horse\", \"Kangaroo\", \"Lion\", \"Panda\",\n",
    "    \"Tiger\", \"Zebra\"\n",
    "]\n",
    "\n",
    "def show_predictions(model, dataloader, num_images=8):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "                image_np = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())  # normalize\n",
    "\n",
    "                plt.subplot(2, int(np.ceil(num_images / 2)), images_shown + 1)\n",
    "                plt.imshow(image_np)\n",
    "                plt.title(f\"✅ True: {class_names[labels[i]]}\\n🔮 Pred: {class_names[preds[i]]}\",\n",
    "                          fontsize=9, color=\"green\" if preds[i] == labels[i] else \"red\")\n",
    "                plt.axis(\"off\")\n",
    "                images_shown += 1\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce096fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test loader\n",
    "_, _, test_loader = get_dataloaders()\n",
    "\n",
    "# Show predictions\n",
    "show_predictions(model, test_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_predictions_one_per_class(model, dataloader, class_names, save_path=\"../reports/predictions_per_class.png\"):\n",
    "    model.eval()\n",
    "    shown_classes = set()\n",
    "    num_classes = len(class_names)\n",
    "    rows, cols = 3, 5 if num_classes <= 15 else (int(np.ceil(num_classes / 5)), 5)\n",
    "\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                true_cls = labels[i].item()\n",
    "                if true_cls in shown_classes:\n",
    "                    continue\n",
    "\n",
    "                img_np = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "                idx = len(shown_classes)\n",
    "                plt.subplot(rows, cols, idx + 1)\n",
    "                plt.imshow(img_np)\n",
    "                plt.title(f\"✅ {class_names[true_cls]}\\n🔮 {class_names[preds[i]]}\",\n",
    "                          color=\"green\" if preds[i] == labels[i] else \"red\", fontsize=9)\n",
    "                plt.axis(\"off\")\n",
    "                shown_classes.add(true_cls)\n",
    "\n",
    "                if len(shown_classes) == num_classes:\n",
    "                    break\n",
    "            if len(shown_classes) == num_classes:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"🖼️ Saved prediction preview grid to: {save_path}\")\n",
    "# Show one prediction per class\n",
    "show_predictions_one_per_class(model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d21efc",
   "metadata": {},
   "source": [
    "### 7. **Soothing and Calming Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da76149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_accuracy_plotly(train_acc, val_acc):\n",
    "    epochs = list(range(1, len(train_acc) + 1))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=train_acc, mode='lines+markers', name='Train Accuracy'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=val_acc, mode='lines+markers', name='Validation Accuracy'))\n",
    "    fig.update_layout(\n",
    "        title=\"📈 Accuracy per Epoch\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()\n",
    "plot_accuracy_plotly(train_acc, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f213d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_classwise_metrics(df):\n",
    "    fig = px.bar(\n",
    "        df.melt(id_vars=\"Class\", value_vars=[\"Precision\", \"Recall\", \"F1-Score\"]),\n",
    "        x=\"Class\", y=\"value\", color=\"variable\", barmode=\"group\",\n",
    "        title=\"🔍 Per-Class Evaluation Metrics\"\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"Score\", template=\"plotly_white\")\n",
    "    fig.show()\n",
    "plot_classwise_metrics(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130869eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.templates[\"custom_mint\"] = go.layout.Template(\n",
    "    layout=dict(\n",
    "        font=dict(family=\"Segoe UI\", size=13, color=\"#333\"),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\",\n",
    "        colorway=[\"#6A5ACD\", \"#2E8B8B\", \"#FF6B6B\", \"#E8C547\", \"#3CB371\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "pio.templates.default = \"custom_mint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9a471",
   "metadata": {},
   "source": [
    "#### Accuracy Curves (Train vs. Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U kaleido\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(go.Bar(y=[3, 2, 1]))\n",
    "fig.write_image(\"test_plot.png\")  # if this runs without error, kaleido is working ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba70c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_accuracy_plotly(train_acc, val_acc):\n",
    "    epochs = list(range(1, len(train_acc) + 1))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=train_acc, mode='lines+markers', name='Train Accuracy'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=val_acc, mode='lines+markers', name='Validation Accuracy'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"📈 Accuracy over Epochs\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        template=\"plotly_white\",\n",
    "        colorway=[\"#6A5ACD\", \"#3CB371\"]\n",
    "    )\n",
    "    fig.write_image(\"../reports/plot_accuracy_curve.svg\", scale=3)\n",
    "    \n",
    "    fig.show()\n",
    "plot_accuracy_plotly(train_acc, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix_plotly(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = np.round(100 * cm / cm.sum(axis=1, keepdims=True), 1)\n",
    "\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=cm_percent,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='BuGn',\n",
    "        showscale=True,\n",
    "        annotation_text=cm.astype(str),\n",
    "        hoverinfo=\"z\"\n",
    "    )\n",
    "    fig.update_layout(title=\"🔍 Confusion Matrix (%)\", xaxis_title=\"Predicted\", yaxis_title=\"Actual\")\n",
    "    fig.write_image(\"../reports/plot_confusion_matrix.png\", scale=3)\n",
    "    fig.show()\n",
    "plot_confusion_matrix_plotly(all_labels, all_preds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_classwise_metrics(df):  # df = your existing results_df\n",
    "    fig = px.bar(\n",
    "        df.melt(id_vars=\"Class\", value_vars=[\"Precision\", \"Recall\", \"F1-Score\"]),\n",
    "        x=\"Class\", y=\"value\", color=\"variable\", barmode=\"group\",\n",
    "        title=\"📊 Per-Class Metrics\",\n",
    "        color_discrete_sequence=[\"#6A5ACD\", \"#3CB371\", \"#FF6B6B\"]\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"Score\", template=\"plotly_white\")\n",
    "    fig.write_image(\"../reports/plot_classwise_metrics.png\", scale=3)\n",
    "    fig.show()\n",
    "\n",
    "plot_classwise_metrics(results_df)  # Use your existing results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df):  # df[\"Images\"] and df[\"Class\"] expected\n",
    "    fig = px.pie(\n",
    "        df,\n",
    "        names=\"Class\",\n",
    "        values=\"Images\",\n",
    "        title=\"🧮 Test Set Class Distribution\",\n",
    "        color_discrete_sequence=px.colors.sequential.Mint\n",
    "    )\n",
    "    fig.update_traces(textinfo=\"percent+label\", pull=[0.02]*len(df))\n",
    "    fig.write_image(\"../reports/plot_class_distribution_pie.png\", scale=3)\n",
    "    fig.show()\n",
    "plot_class_distribution(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions_one_per_class(model, test_loader, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_misclassified_grid(model, dataloader, class_names, n=12, save_path=\"../reports/misclassified_grid.png\"):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "\n",
    "            for img, true_label, pred_label in zip(x, y, pred):\n",
    "                if true_label != pred_label:\n",
    "                    errors.append((img.cpu(), true_label.item(), pred_label.item()))\n",
    "                if len(errors) >= n:\n",
    "                    break\n",
    "            if len(errors) >= n:\n",
    "                break\n",
    "\n",
    "    if not errors:\n",
    "        print(\"🥳 No misclassifications found!\")\n",
    "        return\n",
    "\n",
    "    rows, cols = (n + 3) // 4, 4\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "\n",
    "    for idx, (img, true_idx, pred_idx) in enumerate(errors):\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "        plt.subplot(rows, cols, idx + 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"True: {class_names[true_idx]}\\nPred: {class_names[pred_idx]}\", \n",
    "                  fontsize=9, \n",
    "                  color=\"red\")\n",
    "\n",
    "    plt.suptitle(\"❌ Misclassified Samples\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"🖼️ Saved misclassification grid to: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
